{"rule_id":"entropy_threshold_low_claude_001","model":"claude-haiku-4-5-20251001","few_shot_count":20,"prompt_variation":"simple","ground_truth_articulation":"Text is classified as TRUE if the Shannon entropy of character distribution is below 4.2 bits, suggesting repetitive or predictable character patterns.","generated_articulation":"# Rule:\n\nThe output is **True** when the input is a single word or short exclamation (typically one to two words, often with minimal or no spaces), and **False** when the input is a complete phrase, sentence, or proverb with multiple distinct words.","raw_response":"# Rule:\n\nThe output is **True** when the input is a single word or short exclamation (typically one to two words, often with minimal or no spaces), and **False** when the input is a complete phrase, sentence, or proverb with multiple distinct words.","keyword_match_score":0.0,"rouge_l_score":0.15384615384615383,"llm_judge_score":0.3,"llm_judge_reasoning":"The ground truth rule classifies text as TRUE based on the Shannon entropy of character distribution being below 4.2 bits, which indicates repetitive or predictable character patterns—often associated with short, simple, or repetitive texts. The generated rule, however, classifies based on the length and complexity of the input—specifically, whether it is a single word or short exclamation versus a full phrase or sentence. While both rules tend to favor shorter, simpler inputs as TRUE, they do so based on different criteria: one on character entropy, the other on word count and structure. They do not directly identify the same features or patterns, and their classifications may diverge for inputs that are short but complex (e.g., a short phrase with high entropy) or long but simple (e.g., a repeated word). Therefore, their core concepts are only loosely aligned, leading to limited overlap in classification logic.","functional_test_accuracy":0.88,"functional_test_details":{"n_total":50,"n_classified":50,"n_correct":44,"n_skipped":0,"accuracy":0.88},"parse_error":false}
