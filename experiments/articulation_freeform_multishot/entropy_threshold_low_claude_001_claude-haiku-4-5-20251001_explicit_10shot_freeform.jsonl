{"rule_id":"entropy_threshold_low_claude_001","model":"claude-haiku-4-5-20251001","few_shot_count":10,"prompt_variation":"explicit","ground_truth_articulation":"Text is classified as TRUE if the Shannon entropy of character distribution is below 4.2 bits, suggesting repetitive or predictable character patterns.","generated_articulation":"# Classification Rule\n\n**True**: Single words, interjections, or simple utterances (including single-word answers and exclamations)\n\n**False**: Proverbs, idioms, or multi-word phrases with figurative/proverbial meaning\n\nMore precisely: Classify as **True** if the text is a simple, literal expression (one word or basic utterance). Classify as **False** if the text is a well-known saying, proverb, or idiomatic phrase that conveys wisdom or figurative meaning.","raw_response":"# Classification Rule\n\n**True**: Single words, interjections, or simple utterances (including single-word answers and exclamations)\n\n**False**: Proverbs, idioms, or multi-word phrases with figurative/proverbial meaning\n\nMore precisely: Classify as **True** if the text is a simple, literal expression (one word or basic utterance). Classify as **False** if the text is a well-known saying, proverb, or idiomatic phrase that conveys wisdom or figurative meaning.","keyword_match_score":0.0,"rouge_l_score":0.18181818181818182,"llm_judge_score":0.3,"llm_judge_reasoning":"The ground truth rule bases classification on the Shannon entropy of character distribution, which measures the predictability or repetitiveness of the text at a low-level, quantitative feature. It broadly captures whether the text is simple or complex based on character patterns, likely classifying very repetitive or predictable texts as TRUE and more complex or varied texts as FALSE.\n\nThe generated rule, however, relies on semantic and structural features—distinguishing between simple, literal expressions (single words, interjections) and idiomatic or proverb-like multi-word phrases with figurative meaning. It emphasizes the nature of the content rather than the statistical properties of the text.\n\nWhile there is some overlap—simple, literal expressions are often short and predictable, potentially resulting in low entropy, and idiomatic/proverbial phrases tend to be longer and more varied—the two rules do not directly correspond. The generated rule may classify complex but predictable texts as FALSE, and simple but unpredictable texts as TRUE, which the entropy-based rule would handle differently.\n\nOverall, they target different aspects of the text (statistical predictability vs. semantic/literal simplicity), so their classifications will often differ. Therefore, the core concepts are not equivalent, and the overlap in classification is limited.","functional_test_accuracy":0.9,"functional_test_details":{"n_total":50,"n_classified":50,"n_correct":45,"n_skipped":0,"accuracy":0.9},"parse_error":false}
